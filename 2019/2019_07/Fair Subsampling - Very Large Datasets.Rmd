---
title: "Fair Subsampling from Very Large Datasets"
author: Sheila Braun
date: July 16, 2019
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

```{r set_options}
options(digits = 3, scipen = 9999)
sh <- suppressMessages # it's a library, so sh!
shh <- suppressWarnings
shhh <- suppressPackageStartupMessages
sh(shh(shhh(if(!require(dataPreparation)){install.packages("dataPreparation")})))
sh(shh(shhh(if(!require(fastDummies)){install.packages("fastDummies")})))
sh(shh(shhh(if(!require(gridBase)){install.packages("gridBase")})))
#sh(shh(shhh(if(!require(ggthemr)){install.packages("ggthemr")})))
```

# An Example of Unfairness

Fake data: create a very long vector of greetings

```{r create_data}
greetings = c("Ciao",
              "Nǐn hǎo", 
              "Guten tag",    
              "Bonjour",
              "Olá",
              "Bom dia",
              "Zdravstvuyte",
              "Shalom",
              "Gudmoning!",
              "Aloha")

set.seed(9699)
greeting <- sample(greetings,
                   size = 10000000, 
                   replace = TRUE, 
                   prob = c(10/25,
                            5/25,
                            1/25, 
                            .5/25, 
                            .5/25,
                            .5/25, 
                            .5/25,
                            1/25,  
                            .00001/25,
                     .09999/25))
```

## Summarize the Greetings

```{r show_data}
library(ggplot2)
library(ggthemes)
(my_table <- base::table(greeting))
barplot(my_table, col = rainbow(10), legend = TRUE)
```

## Random Samples of Greetings

```{r subsample}
plot_subsample <- function(greeting, how_many) {
  set.seed(9699)
  library(gridBase)
  barplot(table(sample(greeting, how_many)), 
       main = "Greeting Patients from Around the World", 
       xlab = "Greeting", 
       ylab = "Count",
       col = rainbow(10),
       legend = TRUE)
}

how_many <- 30

plot_subsample(greeting, how_many * 10000)
```


# An Example of Critical Unfairness

This time the data is a list of symptoms of a virus you, the brilliant epidemiologist, have just identified. We'll repeat the randomization to create the data and the descriptive statistics. 


### Symptoms

```{r another_example}
symptoms = c("Coughing",
             "Rash", 
             "Sleeplessness",    
             "Listlessness",
             "Loss of\nAppetite",
             "Sore Throat",
             "Bloody urine",
             "Vomiting",
             "Seizures",
             "Fever")
```


Probability of the symptoms:

```{r}
set.seed(960001)
symptom <- sample(symptoms,
                  size = 10000000, 
                  replace = TRUE, 
                  prob = c(10/25,
                           5/25,
                           1/25, 
                           .5/25, 
                           .5/25,
                           .5/25, 
                           .5/25,
                           1/25,  
                           .00001/25,
                           .09999/25))
```


```{r do_not_look, include = FALSE}
sh(library(tidyverse))
df <- as_tibble(cbind(greeting, symptom))
df <- df %>% mutate(died = "No")
df$id <- sample(10000000:19999999, replace = FALSE)
add_symptom <- function(symptoms) {
  sample(symptoms,
         size = 10000000, 
         replace = TRUE, 
         prob = c(10/25,
                  5/25,
                  1/25, 
                  .5/25, 
                  .5/25,
                  .5/25, 
                  .5/25,
                  1/25,  
                  .00001/25,
                  .09999/25))
}
df$symptom2 <- add_symptom(symptoms)
df$symptom3 <- add_symptom(symptoms)
df$symptom4 <- add_symptom(symptoms)
df 
sh(library(fastDummies))
all_symptoms <- grep(pattern = "symptom", x = names(df), value = TRUE)
sh(library(MASS))
sh(library(reshape2))
sh(library(reshape))
sh(library(data.table))
df <- data.table::melt(df, id = c("id","greeting", "died"))
df <- dplyr::as_tibble(df)
df <- df %>% 
  dplyr::select(-variable) %>% 
  mutate(symptom = value) %>%
  dplyr::select(-value)
df$symptom <- factor(df$symptom) # makes other things run faster
df$died[which(df$greeting == "Gudmoning!")] <- "Yes" 
df$symptom[which(df$greeting == "Gudmoning!")] <- "Seizures"

# Not doing this bit because it takes too long
# df <- unique(df) # go get a cup of tea

# get previously uniqued df
df <- fread("df.csv")

# table(df$greeting, df$symptom)
# data.table::fwrite(df, "df.csv")
```


## How many people died of _virus madeupticus_?

```{r num_died, fig.width = 200}
table(df$died)
```


## Can we predict fatalities?

```{r predict}
sh(shh(shhh(library(dataPreparation))))
# Encode categorical variables
# First create schema
encoding <- build_encoding(dataSet = df, cols = "auto", verbose = TRUE)
# Now change data sets to reflect encoding and have a look
df <- one_hot_encoder(dataSet = df, encoding = encoding, drop = TRUE, verbose = TRUE)

# Create train & test indices
train_index <- sample(1:nrow(df), 0.8 * nrow(df))
test_index <- setdiff(1:nrow(df), train_index)

# Create train & test sets
train <- df[train_index,]
test <- df[test_index,]
```


## Data preparation is done. Now create a model using the training set. 

```{r train}
#model <- lm(died.Yes ~ ., data = train,
#              family=binomial(link="logit"))
# summary(logitMod)

## Output:
## ERROR: FAILED TO CONVERGE
```

Error: Model failed to converge

## What does this mean?

There is no way to predict fatalities.

Right?

It's only 6 people out of 10,000,000. This is not a dangerous illnesss.

Right?


## The problem:

```{r the_problem}
# recover df
df <- data.table::fread("df.csv")
barplot(table(df$greeting, df$died), col = rainbow(10))
table(df$greeting, df$died)
```


# What can you do about it?

1. **Don't take subsets.** Data scientists like to use all the data and there's nothing wrong with that if you can do it.

2. **Exploratory analysis is good--and it's good for you!** We would not have needed to go through all that work if we'd done a few simple tables at the beginning to see how variables interacted.

```{r more_tables}
table(df$symptom, df$died)
table(df$symptom, df$greeting)
```

But sometimes you just can't find anything. You have 250 variables and 10,000,000 cases, for instance. What do you do?

3. **Try Cost-sensitive Learning**. This method evaluates the cost associated with misclassifying data. Use it to decide how important it is to find absolutely all the relationships among your variables.

![Cost-sensitive learning table](/Users/braunsb/Desktop/Screen Shot 2019-07-13 at 6.19.25 PM.png)

4. **Avoid random sampling**. _Random sampling_ is taking a random sample of anyone from any group or subgroup in your data set. it's what I did above. **Why this won't work**: You might not get anybody from Vanuatu. In fact, you almost certainly won't unless you take a hefty portion of the data set in your random sample. 

```{r plotting_symptoms}
plot_subsample <- function(symptom, how_many) {
  set.seed(100)
  barplot(table(sample(symptom, how_many)), 
          main = "Virus Madeupticus Symptoms", 
          xlab = "symptom", 
          ylab = "Count",
          col=rainbow(10), legend = TRUE, 
          cex.names = .50,
          beside = TRUE)
}
how_many <- 42
plot_subsample(symptom, how_many)
plot_subsample(symptom, how_many * 10)
plot_subsample(symptom, how_many * 100)
plot_subsample(symptom, how_many * 1000)
plot_subsample(symptom, how_many * 10000)
plot_subsample(symptom, how_many * 100000)
```


5. **Avoid traditional stratified random sampling**: Stratified random sampling seems like a great idea because you create a subsample that takes a percentage of each group. The percentage is equal to the percentage that group represents in the entire data set. **Why this won't work**: We'd have even fewer than 6 people from Vanuatu in the data set. 

# Solution

1. Use cost-sensitive learning to establish importance

2. Use exploratory data analysis to find all the majorities and all the hidden minorities, then write your descriptive analysis code using the data dictionary to inform it (this makes it easier to include all the important variables). You'll find shortcuts as you go along. Be thorough in this step.

2. Oversample minority groups if you must, as undersampling the majority is better. In cases like ours, however, we must oversample because the minority group is so very small.

5. Undersample the majorities. Read on to find out how. 

### One way to Undersample Majority Groups: Clustering

1. Divide the majority class into a bunch of _distinct_ clusters (each element of the data shows up only once in the clusters). 

3. Train each of these cluster with _all_ observations from minority class, oversampling the minority if that's necessary for them to show up fairly (in our example, we would have to do this). 

4. Finally, average your final predictions to get an accurate prediction.

>When to think about fairness: all the time---especially when it is _critical_ that you account for all the categories in your data set. 

# For more on this problem

* Class Imbalance, and Cost Sensitivity: Why Under-Sampling beats Over-Sampling: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.68.6858&rep=rep1&type=pdf

* The Imbalanced Training Sample Problem: Under or over Sampling? https://link.springer.com/chapter/10.1007/978-3-540-27868-9_88

* SMOTE-RSB *: a hybrid preprocessing approach based on oversampling and undersampling for high imbalanced data-sets using SMOTE and rough sets theory: https://link.springer.com/article/10.1007/s10115-011-0465-6

* Practical Guide to deal with Imbalanced Classification Problems in R: https://www.analyticsvidhya.com/blog/2016/03/practical-guide-deal-imbalanced-classification-problems/ <<== where I got the _cost-sensitive learning_ table.

* [ovun.sample](https://rdrr.io/cran/ROSE/man/ovun.sample.html) is a package that aids in creating fair random samples. 

* [ROSE](https://rdrr.io/cran/ROSE/man/ROSE.html) uses synthetic sampling, which we have not discussed here but which is another option for getting fair predictions. 

